{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Iallen520/lhy_DL_Hw/blob/master/hw4_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r67y9UpchZ38"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "本次作業是要讓同學接觸NLP當中一個簡單的task——句子分類(文本分類)\n",
    "\n",
    "給定一個句子，判斷他有沒有惡意(負面標1，正面標0)\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ajS_WskRo0S"
   },
   "outputs": [],
   "source": [
    "path_prefix = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YrAlczfM_w6"
   },
   "source": [
    "### Download Dataset\n",
    "有三個檔案，分別是training_label.txt、training_nolabel.txt、testing_data.txt\n",
    "\n",
    "training_label.txt：有label的training data(句子配上0 or 1)\n",
    "\n",
    "training_nolabel.txt：沒有label的training data(只有句子)，用來做semi-supervise learning\n",
    "\n",
    "testing_data.txt：你要判斷testing data裡面的句子是0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hDIokoP6464"
   },
   "outputs": [],
   "source": [
    "# this is for filtering the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fc143hSvNGr6"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICDIhhgCY2-M"
   },
   "outputs": [],
   "source": [
    "# utils.py\n",
    "# 這個block用來先定義一些等等常用到的函式\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_training_data(path='training_label.txt'):\n",
    "    # 把training時需要的data讀進來\n",
    "    # 如果是'training_label.txt'，需要讀取label，如果是'training_nolabel.txt'，不需要讀取label\n",
    "    if 'training_label' in path:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
    "        x = [line[2:] for line in lines]\n",
    "        y = [line[0] for line in lines]\n",
    "        return x, y\n",
    "    else:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            x = [line.strip('\\n').split(' ') for line in lines]\n",
    "        return x\n",
    "\n",
    "def load_testing_data(path='testing_data'):\n",
    "    # 把testing時需要的data讀進來\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
    "        X = [sen.split(' ') for sen in X]\n",
    "    return X\n",
    "\n",
    "def evaluation(outputs, labels):\n",
    "    #outputs => probability (float)\n",
    "    #labels => labels\n",
    "    outputs[outputs>=0.5] = 1 # 大於等於0.5為有惡意\n",
    "    outputs[outputs<0.5] = 0 # 小於0.5為無惡意\n",
    "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYE8UYQsNIxM"
   },
   "source": [
    "### Train Word to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "cgGWaF8_2S3q",
    "outputId": "d25efb6b-de4f-43f2-8a98-e0aaa47f704e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data ...\n",
      "loading testing data ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-220b44cf281e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtest_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_testing_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'testing_data.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtrain_x_no_label\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saving model ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-220b44cf281e>\u001b[0m in \u001b[0;36mtrain_word2vec\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# 訓練word to vector 的 word embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# w2v.py\n",
    "# 這個block是用來訓練word to vector 的 word embedding\n",
    "# 注意！這個block在訓練word to vector時是用cpu，可能要花到10分鐘以上\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def train_word2vec(x):\n",
    "    # 訓練word to vector 的 word embedding\n",
    "    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"loading training data ...\")\n",
    "    train_x, y = load_training_data('training_label.txt')\n",
    "    train_x_no_label = load_training_data('training_nolabel.txt')\n",
    "\n",
    "    print(\"loading testing data ...\")\n",
    "    test_x = load_testing_data('testing_data.txt')\n",
    "\n",
    "    model = train_word2vec(train_x + train_x_no_label + test_x)\n",
    "    \n",
    "    print(\"saving model ...\")\n",
    "    model.save(os.path.join(path_prefix, 'w2v_all.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wHLtS0wNR6w"
   },
   "source": [
    "### Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CfGKiOitk5ob"
   },
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "# 這個block用來做data的預處理\n",
    "from torch import nn\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "        \n",
    "    def get_w2v_model(self):\n",
    "        # 把之前訓練好的word to vec 模型讀進來\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "        \n",
    "    def add_embedding(self, word):\n",
    "        # 把word加進embedding，並賦予他一個隨機生成的representation vector\n",
    "        # word只會是\"<PAD>\"或\"<UNK>\"\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "        \n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        # 取得訓練好的 Word2vec word embedding\n",
    "        if load:\n",
    "            print(\"loading word to vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # 製作一個 word2idx 的 dictionary\n",
    "        # 製作一個 idx2word 的 list\n",
    "        # 製作一個 word2vector 的 list\n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "            print('get words #{}'.format(i+1), end='\\r')\n",
    "            #e.g. self.word2index['魯'] = 1 \n",
    "            #e.g. self.index2word[1] = '魯'\n",
    "            #e.g. self.vectors[1] = '魯' vector\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        # 將\"<PAD>\"跟\"<UNK>\"加進embedding裡面\n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "    \n",
    "    def pad_sequence(self, sentence):\n",
    "        # 將每個句子變成一樣的長度\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "    \n",
    "    def sentence_word2idx(self):\n",
    "        # 把句子裡面的字轉成相對應的index\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            print('sentence count #{}'.format(i+1), end='\\r')\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if word in self.word2idx.keys():\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "            # 將每個句子變成一樣的長度\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "    \n",
    "    def labels_to_tensor(self, y):\n",
    "        # 把labels轉成tensor\n",
    "        y = [int(label) for label in y]\n",
    "        return torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WJB7go5NWL0"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XketwKs4lFfB"
   },
   "outputs": [],
   "source": [
    "# data.py\n",
    "# 實作了dataset所需要的'__init__', '__getitem__', '__len__'\n",
    "# 好讓dataloader能使用\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class TwitterDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Expected data shape like:(data_num, data_len)\n",
    "    Data can be a list of numpy array or a list of lists\n",
    "    input data shape : (data_num, seq_len, feature_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: return self.data[idx]\n",
    "        return self.data[idx], self.label[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNJ8xWIMNa2r"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZS6RJADulIq1"
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "# 這個block是要拿來訓練的模型\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        # 製作 embedding layer\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0), embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        # 是否將 embedding fix住，如果fix_embedding為False，在訓練過程中，embedding也會跟著被訓練\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
    "                                         nn.Linear(hidden_dim, 1),\n",
    "                                         nn.Sigmoid() )\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        # x 的 dimension (batch, seq_len, hidden_size)\n",
    "        # 取用 LSTM 最後一層的 hidden state\n",
    "        x = x[:, -1, :] \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aWlpEL0sNc10"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QR4MMz-lR7i"
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "# 這個block是用來訓練模型的\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
    "    model.train() # 將model的模式設為train，這樣optimizer就可以更新model的參數\n",
    "    criterion = nn.BCELoss() # 定義損失函數，這裡我們使用binary cross entropy loss\n",
    "    t_batch = len(train) \n",
    "    v_batch = len(valid) \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # 將模型的參數給optimizer，並給予適當的learning rate\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        # 這段做training\n",
    "        for i, (inputs, labels) in enumerate(train):\n",
    "            inputs = inputs.to(device, dtype=torch.long) # device為\"cuda\"，將inputs轉成torch.cuda.LongTensor\n",
    "            labels = labels.to(device, dtype=torch.float) # device為\"cuda\"，將labels轉成torch.cuda.FloatTensor，因為等等要餵進criterion，所以型態要是float\n",
    "            optimizer.zero_grad() # 由於loss.backward()的gradient會累加，所以每次餵完一個batch後需要歸零\n",
    "            outputs = model(inputs) # 將input餵給模型\n",
    "            outputs = outputs.squeeze() # 去掉最外面的dimension，好讓outputs可以餵進criterion()\n",
    "            loss = criterion(outputs, labels) # 計算此時模型的training loss\n",
    "            loss.backward() # 算loss的gradient\n",
    "            optimizer.step() # 更新訓練模型的參數\n",
    "            correct = evaluation(outputs, labels) # 計算此時模型的training accuracy\n",
    "            total_acc += (correct / batch_size)\n",
    "            total_loss += loss.item()\n",
    "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
    "            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
    "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
    "\n",
    "        # 這段做validation\n",
    "        model.eval() # 將model的模式設為eval，這樣model的參數就會固定住\n",
    "        with torch.no_grad():\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (inputs, labels) in enumerate(valid):\n",
    "                inputs = inputs.to(device, dtype=torch.long) # device為\"cuda\"，將inputs轉成torch.cuda.LongTensor\n",
    "                labels = labels.to(device, dtype=torch.float) # device為\"cuda\"，將labels轉成torch.cuda.FloatTensor，因為等等要餵進criterion，所以型態要是float\n",
    "                outputs = model(inputs) # 將input餵給模型\n",
    "                outputs = outputs.squeeze() # 去掉最外面的dimension，好讓outputs可以餵進criterion()\n",
    "                loss = criterion(outputs, labels) # 計算此時模型的validation loss\n",
    "                correct = evaluation(outputs, labels) # 計算此時模型的validation accuracy\n",
    "                total_acc += (correct / batch_size)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "            if total_acc > best_acc:\n",
    "                # 如果validation的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\n",
    "                best_acc = total_acc\n",
    "                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n",
    "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
    "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
    "        print('-----------------------------------------------')\n",
    "        model.train() # 將model的模式設為train，這樣optimizer就可以更新model的參數（因為剛剛轉成eval模式）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qF5YQrupNfCS"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2X2wkdAYxHYA"
   },
   "outputs": [],
   "source": [
    "# test.py\n",
    "# 這個block用來對testing_data.txt做預測\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def testing(batch_size, test_loader, model, device):\n",
    "    model.eval()\n",
    "    ret_output = []\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            outputs[outputs>=0.5] = 1 # 大於等於0.5為負面\n",
    "            outputs[outputs<0.5] = 0 # 小於0.5為正面\n",
    "            ret_output += outputs.int().tolist()\n",
    "    \n",
    "    return ret_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfnKj0KXNeoz"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "EztIWqCmlZof",
    "outputId": "961deee1-4726-445b-cd49-ab4fcc0e14cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n",
      "get words #55777\n",
      "total words: 55779\n",
      "sentence count #200000\n",
      "start training, parameter total:14447001, trainable:502251\n",
      "\n",
      "\n",
      "Train | Loss:0.48037 Acc: 76.344\n",
      "Valid | Loss:0.43569 Acc: 79.391 \n",
      "saving model with acc 79.391\n",
      "-----------------------------------------------\n",
      "[ Epoch2: 1485/1485 ] loss:0.445 acc:28.906 \n",
      "Train | Loss:0.41297 Acc: 81.141\n",
      "Valid | Loss:0.40639 Acc: 80.736 \n",
      "saving model with acc 80.736\n",
      "-----------------------------------------------\n",
      "\n",
      "Train | Loss:0.39345 Acc: 82.240\n",
      "Valid | Loss:0.39597 Acc: 81.517 \n",
      "saving model with acc 81.517\n",
      "-----------------------------------------------\n",
      "[ Epoch4: 1485/1485 ] loss:0.457 acc:30.469 \n",
      "Train | Loss:0.37718 Acc: 83.130\n",
      "Valid | Loss:0.39099 Acc: 81.646 \n",
      "saving model with acc 81.646\n",
      "-----------------------------------------------\n",
      "\n",
      "Train | Loss:0.36063 Acc: 83.993\n",
      "Valid | Loss:0.39466 Acc: 81.695 \n",
      "saving model with acc 81.695\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 通過torch.cuda.is_available()的回傳值進行判斷是否有使用GPU的環境，如果有的話device就設為\"cuda\"，沒有的話就設為\"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 處理好各個data的路徑\n",
    "train_with_label = os.path.join(path_prefix, 'training_label.txt')\n",
    "train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n",
    "testing_data = os.path.join(path_prefix, 'testing_data.txt')\n",
    "\n",
    "w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 處理word to vec model的路徑\n",
    "\n",
    "# 定義句子長度、要不要固定embedding、batch大小、要訓練幾個epoch、learning rate的值、model的資料夾路徑\n",
    "sen_len = 30\n",
    "fix_embedding = True # fix embedding during training\n",
    "batch_size = 128\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "# model_dir = os.path.join(path_prefix, 'model/') # model directory for checkpoint model\n",
    "model_dir = path_prefix # model directory for checkpoint model\n",
    "\n",
    "print(\"loading data ...\") # 把'training_label.txt'跟'training_nolabel.txt'讀進來\n",
    "train_x, y = load_training_data(train_with_label)\n",
    "train_x_no_label = load_training_data(train_no_label)\n",
    "\n",
    "# 對input跟labels做預處理\n",
    "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "train_x = preprocess.sentence_word2idx()\n",
    "y = preprocess.labels_to_tensor(y)\n",
    "\n",
    "# 製作一個model的對象\n",
    "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=250, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
    "model = model.to(device) # device為\"cuda\"，model使用GPU來訓練(餵進去的inputs也需要是cuda tensor)\n",
    "\n",
    "# 把data分為training data跟validation data(將一部份training data拿去當作validation data)\n",
    "X_train, X_val, y_train, y_val = train_x[:190000], train_x[190000:], y[:190000], y[190000:]\n",
    "\n",
    "# 把data做成dataset供dataloader取用\n",
    "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
    "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
    "\n",
    "# 把data 轉成 batch of tensors\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True,\n",
    "                                            num_workers = 8)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 8)\n",
    "\n",
    "# 開始訓練\n",
    "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fQeaQNeNm3L"
   },
   "source": [
    "### Predict and Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "vFvjFQopxVrt",
    "outputId": "3bf30696-bf76-463b-fc7f-f0b6375a0e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testing data ...\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get words #1\r",
      "get words #2\r",
      "get words #3\r",
      "get words #4\r",
      "get words #5\r",
      "get words #6\r",
      "get words #7\r",
      "get words #8\r",
      "get words #9\r",
      "get words #10\r",
      "get words #11\r",
      "get words #12\r",
      "get words #13\r",
      "get words #14\r",
      "get words #15\r",
      "get words #16\r",
      "get words #17\r",
      "get words #18\r",
      "get words #19\r",
      "get words #20\r",
      "get words #21\r",
      "get words #22\r",
      "get words #23\r",
      "get words #24\r",
      "get words #25\r",
      "get words #26\r",
      "get words #27\r",
      "get words #28\r",
      "get words #29\r",
      "get words #30\r",
      "get words #31\r",
      "get words #32\r",
      "get words #33\r",
      "get words #34\r",
      "get words #35\r",
      "get words #36\r",
      "get words #37\r",
      "get words #38\r",
      "get words #39\r",
      "get words #40\r",
      "get words #41\r",
      "get words #42\r",
      "get words #43\r",
      "get words #44\r",
      "get words #45\r",
      "get words #46\r",
      "get words #47\r",
      "get words #48\r",
      "get words #49\r",
      "get words #50\r",
      "get words #51\r",
      "get words #52\r",
      "get words #53\r",
      "get words #54\r",
      "get words #55\r",
      "get words #56\r",
      "get words #57\r",
      "get words #58\r",
      "get words #59\r",
      "get words #60\r",
      "get words #61\r",
      "get words #62\r",
      "get words #63\r",
      "get words #64\r",
      "get words #65\r",
      "get words #66\r",
      "get words #67\r",
      "get words #68\r",
      "get words #69\r",
      "get words #70\r",
      "get words #71\r",
      "get words #72\r",
      "get words #73\r",
      "get words #74\r",
      "get words #75\r",
      "get words #76\r",
      "get words #77\r",
      "get words #78\r",
      "get words #79\r",
      "get words #80\r",
      "get words #81\r",
      "get words #82\r",
      "get words #83\r",
      "get words #84\r",
      "get words #85\r",
      "get words #86\r",
      "get words #87\r",
      "get words #88\r",
      "get words #89\r",
      "get words #90\r",
      "get words #91\r",
      "get words #92\r",
      "get words #93\r",
      "get words #94\r",
      "get words #95\r",
      "get words #96\r",
      "get words #97\r",
      "get words #98\r",
      "get words #99\r",
      "get words #100\r",
      "get words #101\r",
      "get words #102\r",
      "get words #103\r",
      "get words #104\r",
      "get words #105\r",
      "get words #106\r",
      "get words #107\r",
      "get words #108\r",
      "get words #109\r",
      "get words #110\r",
      "get words #111\r",
      "get words #112\r",
      "get words #113\r",
      "get words #114\r",
      "get words #115\r",
      "get words #116\r",
      "get words #117\r",
      "get words #118\r",
      "get words #119\r",
      "get words #120\r",
      "get words #121\r",
      "get words #122\r",
      "get words #123\r",
      "get words #124\r",
      "get words #125\r",
      "get words #126\r",
      "get words #127\r",
      "get words #128\r",
      "get words #129\r",
      "get words #130\r",
      "get words #131\r",
      "get words #132\r",
      "get words #133\r",
      "get words #134\r",
      "get words #135\r",
      "get words #136\r",
      "get words #137\r",
      "get words #138\r",
      "get words #139\r",
      "get words #140\r",
      "get words #141\r",
      "get words #142\r",
      "get words #143\r",
      "get words #144\r",
      "get words #145\r",
      "get words #146\r",
      "get words #147\r",
      "get words #148\r",
      "get words #149\r",
      "get words #150\r",
      "get words #151\r",
      "get words #152\r",
      "get words #153\r",
      "get words #154\r",
      "get words #155\r",
      "get words #156\r",
      "get words #157\r",
      "get words #158\r",
      "get words #159\r",
      "get words #160\r",
      "get words #161\r",
      "get words #162\r",
      "get words #163\r",
      "get words #164\r",
      "get words #165\r",
      "get words #166\r",
      "get words #167\r",
      "get words #168\r",
      "get words #169\r",
      "get words #170\r",
      "get words #171\r",
      "get words #172\r",
      "get words #173\r",
      "get words #174\r",
      "get words #175\r",
      "get words #176\r",
      "get words #177\r",
      "get words #178\r",
      "get words #179\r",
      "get words #180\r",
      "get words #181\r",
      "get words #182\r",
      "get words #183\r",
      "get words #184\r",
      "get words #185\r",
      "get words #186\r",
      "get words #187\r",
      "get words #188\r",
      "get words #189\r",
      "get words #190\r",
      "get words #191\r",
      "get words #192\r",
      "get words #193\r",
      "get words #194\r",
      "get words #195\r",
      "get words #196\r",
      "get words #197\r",
      "get words #198\r",
      "get words #199\r",
      "get words #200\r",
      "get words #201\r",
      "get words #202\r",
      "get words #203\r",
      "get words #204\r",
      "get words #205\r",
      "get words #206\r",
      "get words #207\r",
      "get words #208\r",
      "get words #209\r",
      "get words #210\r",
      "get words #211\r",
      "get words #212\r",
      "get words #213\r",
      "get words #214\r",
      "get words #215\r",
      "get words #216\r",
      "get words #217\r",
      "get words #218\r",
      "get words #219\r",
      "get words #220\r",
      "get words #221\r",
      "get words #222\r",
      "get words #223\r",
      "get words #224\r",
      "get words #225\r",
      "get words #226\r",
      "get words #227\r",
      "get words #228\r",
      "get words #229\r",
      "get words #230\r",
      "get words #231\r",
      "get words #232\r",
      "get words #233\r",
      "get words #234\r",
      "get words #235\r",
      "get words #236\r",
      "get words #237\r",
      "get words #238\r",
      "get words #239\r",
      "get words #240\r",
      "get words #241\r",
      "get words #242\r",
      "get words #243\r",
      "get words #244\r",
      "get words #245\r",
      "get words #246\r",
      "get words #247\r",
      "get words #248\r",
      "get words #249\r",
      "get words #250\r",
      "get words #251\r",
      "get words #252\r",
      "get words #253\r",
      "get words #254\r",
      "get words #255\r",
      "get words #256\r",
      "get words #257\r",
      "get words #258\r",
      "get words #259\r",
      "get words #260\r",
      "get words #261\r",
      "get words #262\r",
      "get words #263\r",
      "get words #264\r",
      "get words #265\r",
      "get words #266\r",
      "get words #267\r",
      "get words #268\r",
      "get words #269\r",
      "get words #270\r",
      "get words #271\r",
      "get words #272\r",
      "get words #273\r",
      "get words #274\r",
      "get words #275\r",
      "get words #276\r",
      "get words #277\r",
      "get words #278\r",
      "get words #279\r",
      "get words #280\r",
      "get words #281\r",
      "get words #282\r",
      "get words #283\r",
      "get words #284\r",
      "get words #285\r",
      "get words #286\r",
      "get words #287\r",
      "get words #288\r",
      "get words #289\r",
      "get words #290\r",
      "get words #291\r",
      "get words #292\r",
      "get words #293\r",
      "get words #294\r",
      "get words #295\r",
      "get words #296\r",
      "get words #297\r",
      "get words #298\r",
      "get words #299\r",
      "get words #300\r",
      "get words #301\r",
      "get words #302\r",
      "get words #303\r",
      "get words #304\r",
      "get words #305\r",
      "get words #306\r",
      "get words #307\r",
      "get words #308\r",
      "get words #309\r",
      "get words #310\r",
      "get words #311\r",
      "get words #312\r",
      "get words #313\r",
      "get words #314\r",
      "get words #315\r",
      "get words #316\r",
      "get words #317\r",
      "get words #318\r",
      "get words #319\r",
      "get words #320\r",
      "get words #321\r",
      "get words #322\r",
      "get words #323\r",
      "get words #324\r",
      "get words #325\r",
      "get words #326\r",
      "get words #327\r",
      "get words #328\r",
      "get words #329\r",
      "get words #330\r",
      "get words #331\r",
      "get words #332\r",
      "get words #333\r",
      "get words #334\r",
      "get words #335\r",
      "get words #336\r",
      "get words #337\r",
      "get words #338\r",
      "get words #339\r",
      "get words #340\r",
      "get words #341\r",
      "get words #342\r",
      "get words #343\r",
      "get words #344\r",
      "get words #345\r",
      "get words #346\r",
      "get words #347\r",
      "get words #348\r",
      "get words #349\r",
      "get words #350\r",
      "get words #351\r",
      "get words #352\r",
      "get words #353\r",
      "get words #354\r",
      "get words #355\r",
      "get words #356\r",
      "get words #357\r",
      "get words #358\r",
      "get words #359\r",
      "get words #360\r",
      "get words #361\r",
      "get words #362\r",
      "get words #363\r",
      "get words #364\r",
      "get words #365\r",
      "get words #366\r",
      "get words #367\r",
      "get words #368\r",
      "get words #369\r",
      "get words #370\r",
      "get words #371\r",
      "get words #372\r",
      "get words #373\r",
      "get words #374\r",
      "get words #375\r",
      "get words #376\r",
      "get words #377\r",
      "get words #378\r",
      "get words #379\r",
      "get words #380\r",
      "get words #381\r",
      "get words #382\r",
      "get words #383\r",
      "get words #384\r",
      "get words #385\r",
      "get words #386\r",
      "get words #387\r",
      "get words #388\r",
      "get words #389\r",
      "get words #390\r",
      "get words #391\r",
      "get words #392\r",
      "get words #393\r",
      "get words #394\r",
      "get words #395\r",
      "get words #396\r",
      "get words #397\r",
      "get words #398\r",
      "get words #399\r",
      "get words #400\r",
      "get words #401\r",
      "get words #402\r",
      "get words #403\r",
      "get words #404\r",
      "get words #405\r",
      "get words #406\r",
      "get words #407\r",
      "get words #408\r",
      "get words #409\r",
      "get words #410\r",
      "get words #411\r",
      "get words #412\r",
      "get words #413\r",
      "get words #414\r",
      "get words #415\r",
      "get words #416\r",
      "get words #417\r",
      "get words #418\r",
      "get words #419\r",
      "get words #420\r",
      "get words #421\r",
      "get words #422\r",
      "get words #423\r",
      "get words #424\r",
      "get words #425\r",
      "get words #426\r",
      "get words #427\r",
      "get words #428\r",
      "get words #429\r",
      "get words #430\r",
      "get words #431\r",
      "get words #432\r",
      "get words #433\r",
      "get words #434\r",
      "get words #435\r",
      "get words #436\r",
      "get words #437\r",
      "get words #438\r",
      "get words #439\r",
      "get words #440\r",
      "get words #441\r",
      "get words #442\r",
      "get words #443\r",
      "get words #444\r",
      "get words #445\r",
      "get words #446\r",
      "get words #447\r",
      "get words #448\r",
      "get words #449\r",
      "get words #450\r",
      "get words #451\r",
      "get words #452\r",
      "get words #453\r",
      "get words #454\r",
      "get words #455\r",
      "get words #456\r",
      "get words #457\r",
      "get words #458\r",
      "get words #459\r",
      "get words #460\r",
      "get words #461\r",
      "get words #462\r",
      "get words #463\r",
      "get words #464\r",
      "get words #465\r",
      "get words #466\r",
      "get words #467\r",
      "get words #468\r",
      "get words #469\r",
      "get words #470\r",
      "get words #471\r",
      "get words #472\r",
      "get words #473\r",
      "get words #474\r",
      "get words #475\r",
      "get words #476\r",
      "get words #477\r",
      "get words #478\r",
      "get words #479\r",
      "get words #480\r",
      "get words #481\r",
      "get words #482\r",
      "get words #483\r",
      "get words #484\r",
      "get words #485\r",
      "get words #486\r",
      "get words #487\r",
      "get words #488\r",
      "get words #489\r",
      "get words #490\r",
      "get words #491\r",
      "get words #492\r",
      "get words #493\r",
      "get words #494\r",
      "get words #495\r",
      "get words #496\r",
      "get words #497\r",
      "get words #498\r",
      "get words #499\r",
      "get words #500\r",
      "get words #501\r",
      "get words #502\r",
      "get words #503\r",
      "get words #504\r",
      "get words #505\r",
      "get words #506\r",
      "get words #507\r",
      "get words #508\r",
      "get words #509\r",
      "get words #510\r",
      "get words #511\r",
      "get words #512\r",
      "get words #513\r",
      "get words #514\r",
      "get words #515\r",
      "get words #516\r",
      "get words #517\r",
      "get words #518\r",
      "get words #519\r",
      "get words #520\r",
      "get words #521\r",
      "get words #522\r",
      "get words #523\r",
      "get words #524\r",
      "get words #525\r",
      "get words #526\r",
      "get words #527\r",
      "get words #528\r",
      "get words #529\r",
      "get words #530\r",
      "get words #531\r",
      "get words #532\r",
      "get words #533\r",
      "get words #534\r",
      "get words #535\r",
      "get words #536\r",
      "get words #537\r",
      "get words #538\r",
      "get words #539\r",
      "get words #540\r",
      "get words #541\r",
      "get words #542\r",
      "get words #543\r",
      "get words #544\r",
      "get words #545\r",
      "get words #546\r",
      "get words #547\r",
      "get words #548\r",
      "get words #549\r",
      "get words #550\r",
      "get words #551\r",
      "get words #552\r",
      "get words #553\r",
      "get words #554\r",
      "get words #555\r",
      "get words #556\r",
      "get words #557\r",
      "get words #558\r",
      "get words #559\r",
      "get words #560\r",
      "get words #561\r",
      "get words #562\r",
      "get words #563\r",
      "get words #564\r",
      "get words #565\r",
      "get words #566\r",
      "get words #567\r",
      "get words #568\r",
      "get words #569\r",
      "get words #570\r",
      "get words #571\r",
      "get words #572\r",
      "get words #573\r",
      "get words #574\r",
      "get words #575\r",
      "get words #576\r",
      "get words #577\r",
      "get words #578\r",
      "get words #579\r",
      "get words #580\r",
      "get words #581\r",
      "get words #582\r",
      "get words #583\r",
      "get words #584\r",
      "get words #585\r",
      "get words #586\r",
      "get words #587\r",
      "get words #588\r",
      "get words #589\r",
      "get words #590\r",
      "get words #591\r",
      "get words #592\r",
      "get words #593\r",
      "get words #594\r",
      "get words #595\r",
      "get words #596\r",
      "get words #597\r",
      "get words #598\r",
      "get words #599\r",
      "get words #600\r",
      "get words #601\r",
      "get words #602\r",
      "get words #603\r",
      "get words #604\r",
      "get words #605\r",
      "get words #606\r",
      "get words #607\r",
      "get words #608\r",
      "get words #609\r",
      "get words #610\r",
      "get words #611\r",
      "get words #612\r",
      "get words #613\r",
      "get words #614\r",
      "get words #615\r",
      "get words #616\r",
      "get words #617\r",
      "get words #618\r",
      "get words #619\r",
      "get words #620\r",
      "get words #621\r",
      "get words #622\r",
      "get words #623\r",
      "get words #624\r",
      "get words #625\r",
      "get words #626\r",
      "get words #627\r",
      "get words #628\r",
      "get words #629\r",
      "get words #630\r",
      "get words #631\r",
      "get words #632\r",
      "get words #633\r",
      "get words #634\r",
      "get words #635\r",
      "get words #636\r",
      "get words #637\r",
      "get words #638\r",
      "get words #639\r",
      "get words #640\r",
      "get words #641\r",
      "get words #642\r",
      "get words #643\r",
      "get words #644\r",
      "get words #645\r",
      "get words #646\r",
      "get words #647\r",
      "get words #648\r",
      "get words #649\r",
      "get words #650\r",
      "get words #651\r",
      "get words #652\r",
      "get words #653\r",
      "get words #654\r",
      "get words #655\r",
      "get words #656\r",
      "get words #657\r",
      "get words #658\r",
      "get words #659\r",
      "get words #660\r",
      "get words #661\r",
      "get words #662\r",
      "get words #663\r",
      "get words #664\r",
      "get words #665\r",
      "get words #666\r",
      "get words #667\r",
      "get words #668\r",
      "get words #669\r",
      "get words #670\r",
      "get words #671\r",
      "get words #672\r",
      "get words #673\r",
      "get words #674\r",
      "get words #675\r",
      "get words #676\r",
      "get words #677\r",
      "get words #678\r",
      "get words #679\r",
      "get words #680\r",
      "get words #681\r",
      "get words #682\r",
      "get words #683\r",
      "get words #684\r",
      "get words #685\r",
      "get words #686\r",
      "get words #687\r",
      "get words #688\r",
      "get words #689\r",
      "get words #690\r",
      "get words #691\r",
      "get words #692\r",
      "get words #693\r",
      "get words #694\r",
      "get words #695\r",
      "get words #696\r",
      "get words #697\r",
      "get words #698\r",
      "get words #699\r",
      "get words #700\r",
      "get words #701\r",
      "get words #702\r",
      "get words #703\r",
      "get words #704\r",
      "get words #705\r",
      "get words #706\r",
      "get words #707\r",
      "get words #708\r",
      "get words #709\r",
      "get words #710\r",
      "get words #711\r",
      "get words #712\r",
      "get words #713\r",
      "get words #714\r",
      "get words #715\r",
      "get words #716\r",
      "get words #717\r",
      "get words #718\r",
      "get words #719\r",
      "get words #720\r",
      "get words #721\r",
      "get words #722\r",
      "get words #723\r",
      "get words #724\r",
      "get words #725\r",
      "get words #726\r",
      "get words #727\r",
      "get words #728\r",
      "get words #729\r",
      "get words #730\r",
      "get words #731\r",
      "get words #732\r",
      "get words #733\r",
      "get words #734\r",
      "get words #735\r",
      "get words #736\r",
      "get words #737\r",
      "get words #738\r",
      "get words #739\r",
      "get words #740\r",
      "get words #741\r",
      "get words #742\r",
      "get words #743\r",
      "get words #744\r",
      "get words #745\r",
      "get words #746\r",
      "get words #747\r",
      "get words #748\r",
      "get words #749\r",
      "get words #750\r",
      "get words #751\r",
      "get words #752\r",
      "get words #753\r",
      "get words #754\r",
      "get words #755\r",
      "get words #756\r",
      "get words #757\r",
      "get words #758\r",
      "get words #759\r",
      "get words #760\r",
      "get words #761\r",
      "get words #762\r",
      "get words #763\r",
      "get words #764\r",
      "get words #765\r",
      "get words #766\r",
      "get words #767\r",
      "get words #768\r",
      "get words #769\r",
      "get words #770\r",
      "get words #771\r",
      "get words #772\r",
      "get words #773\r",
      "get words #774\r",
      "get words #775\r",
      "get words #776\r",
      "get words #777\r",
      "get words #778\r",
      "get words #779\r",
      "get words #780\r",
      "get words #781\r",
      "get words #782\r",
      "get words #783\r",
      "get words #784\r",
      "get words #785\r",
      "get words #786\r",
      "get words #787\r",
      "get words #788\r",
      "get words #789\r",
      "get words #790\r",
      "get words #791\r",
      "get words #792\r",
      "get words #793\r",
      "get words #794\r",
      "get words #795\r",
      "get words #796\r",
      "get words #797\r",
      "get words #798\r",
      "get words #799\r",
      "get words #800\r",
      "get words #801\r",
      "get words #802\r",
      "get words #803\r",
      "get words #804\r",
      "get words #805\r",
      "get words #806\r",
      "get words #807\r",
      "get words #808\r",
      "get words #809\r",
      "get words #810\r",
      "get words #811\r",
      "get words #812\r",
      "get words #813\r",
      "get words #814\r",
      "get words #815\r",
      "get words #816\r",
      "get words #817\r",
      "get words #818\r",
      "get words #819\r",
      "get words #820\r",
      "get words #821\r",
      "get words #822\r",
      "get words #823\r",
      "get words #824\r",
      "get words #825\r",
      "get words #826\r",
      "get words #827\r",
      "get words #828\r",
      "get words #829\r",
      "get words #830\r",
      "get words #831\r",
      "get words #832\r",
      "get words #833\r",
      "get words #834\r",
      "get words #835\r",
      "get words #836\r",
      "get words #837\r",
      "get words #838\r",
      "get words #839\r",
      "get words #840\r",
      "get words #841\r",
      "get words #842\r",
      "get words #843\r",
      "get words #844\r",
      "get words #845\r",
      "get words #846\r",
      "get words #847\r",
      "get words #848\r",
      "get words #849\r",
      "get words #850\r",
      "get words #851\r",
      "get words #852\r",
      "get words #853\r",
      "get words #854\r",
      "get words #855\r",
      "get words #856\r",
      "get words #857\r",
      "get words #858\r",
      "get words #859\r",
      "get words #860\r",
      "get words #861\r",
      "get words #862\r",
      "get words #863\r",
      "get words #864\r",
      "get words #865\r",
      "get words #866\r",
      "get words #867\r",
      "get words #868\r",
      "get words #869\r",
      "get words #870\r",
      "get words #871\r",
      "get words #872\r",
      "get words #873\r",
      "get words #874\r",
      "get words #875\r",
      "get words #876\r",
      "get words #877\r",
      "get words #878\r",
      "get words #879\r",
      "get words #880\r",
      "get words #881\r",
      "get words #882\r",
      "get words #883\r",
      "get words #884\r",
      "get words #885\r",
      "get words #886\r",
      "get words #887\r",
      "get words #888\r",
      "get words #889\r",
      "get words #890\r",
      "get words #891\r",
      "get words #892\r",
      "get words #893\r",
      "get words #894\r",
      "get words #895\r",
      "get words #896\r",
      "get words #897\r",
      "get words #898\r",
      "get words #899\r",
      "get words #900\r",
      "get words #901\r",
      "get words #902\r",
      "get words #903\r",
      "get words #904\r",
      "get words #905\r",
      "get words #906\r",
      "get words #907\r",
      "get words #908\r",
      "get words #909\r",
      "get words #910\r",
      "get words #911\r",
      "get words #912\r",
      "get words #913\r",
      "get words #914\r",
      "get words #915\r",
      "get words #916\r",
      "get words #917\r",
      "get words #918\r",
      "get words #919\r",
      "get words #920\r",
      "get words #921\r",
      "get words #922\r",
      "get words #923\r",
      "get words #924\r",
      "get words #925\r",
      "get words #926\r",
      "get words #927\r",
      "get words #928\r",
      "get words #929\r",
      "get words #930\r",
      "get words #931\r",
      "get words #932\r",
      "get words #933\r",
      "get words #934\r",
      "get words #935\r",
      "get words #936\r",
      "get words #937\r",
      "get words #938\r",
      "get words #939\r",
      "get words #940\r",
      "get words #941\r",
      "get words #942\r",
      "get words #943\r",
      "get words #944\r",
      "get words #945\r",
      "get words #946\r",
      "get words #947\r",
      "get words #948\r",
      "get words #949\r",
      "get words #950\r",
      "get words #951\r",
      "get words #952\r",
      "get words #953\r",
      "get words #954\r",
      "get words #955\r",
      "get words #956\r",
      "get words #957\r",
      "get words #958\r",
      "get words #959\r",
      "get words #960\r",
      "get words #961\r",
      "get words #962\r",
      "get words #963\r",
      "get words #964\r",
      "get words #965\r",
      "get words #966\r",
      "get words #967\r",
      "get words #968\r",
      "get words #969\r",
      "get words #970\r",
      "get words #971\r",
      "get words #972\r",
      "get words #973\r",
      "get words #974\r",
      "get words #975\r",
      "get words #976\r",
      "get words #977\r",
      "get words #978\r",
      "get words #979\r",
      "get words #980\r",
      "get words #981\r",
      "get words #982\r",
      "get words #983\r",
      "get words #984\r",
      "get words #985\r",
      "get words #986\r",
      "get words #987\r",
      "get words #988\r",
      "get words #989\r",
      "get words #990\r",
      "get words #991\r",
      "get words #992\r",
      "get words #993\r",
      "get words #994\r",
      "get words #995\r",
      "get words #996\r",
      "get words #997\r",
      "get words #998\r",
      "get words #999\r",
      "get words #1000\r",
      "get words #1001\r",
      "get words #1002\r",
      "get words #1003\r",
      "get words #1004\r",
      "get words #1005\r",
      "get words #1006\r",
      "get words #1007\r",
      "get words #1008\r",
      "get words #1009\r",
      "get words #1010\r",
      "get words #1011\r",
      "get words #1012\r",
      "get words #1013\r",
      "get words #1014\r",
      "get words #1015\r",
      "get words #1016\r",
      "get words #1017\r",
      "get words #1018\r",
      "get words #1019\r",
      "get words #1020\r",
      "get words #1021\r",
      "get words #1022\r",
      "get words #1023\r",
      "get words #1024\r",
      "get words #1025\r",
      "get words #1026\r",
      "get words #1027\r",
      "get words #1028\r",
      "get words #1029\r",
      "get words #1030\r",
      "get words #1031\r",
      "get words #1032\r",
      "get words #1033\r",
      "get words #1034\r",
      "get words #1035\r",
      "get words #1036\r",
      "get words #1037\r",
      "get words #1038\r",
      "get words #1039\r",
      "get words #1040\r",
      "get words #1041\r",
      "get words #1042\r",
      "get words #1043\r",
      "get words #1044\r",
      "get words #1045\r",
      "get words #1046\r",
      "get words #1047\r",
      "get words #1048\r",
      "get words #1049\r",
      "get words #1050\r",
      "get words #1051\r",
      "get words #1052\r",
      "get words #1053\r",
      "get words #1054\r",
      "get words #1055\r",
      "get words #1056\r",
      "get words #1057\r",
      "get words #1058\r",
      "get words #1059\r",
      "get words #1060\r",
      "get words #1061\r",
      "get words #1062\r",
      "get words #1063\r",
      "get words #1064\r",
      "get words #1065\r",
      "get words #1066\r",
      "get words #1067\r",
      "get words #1068\r",
      "get words #1069\r",
      "get words #1070\r",
      "get words #1071\r",
      "get words #1072\r",
      "get words #1073\r",
      "get words #1074\r",
      "get words #1075\r",
      "get words #1076\r",
      "get words #1077\r",
      "get words #1078\r",
      "get words #1079\r",
      "get words #1080\r",
      "get words #1081\r",
      "get words #1082\r",
      "get words #1083\r",
      "get words #1084\r",
      "get words #1085\r",
      "get words #1086\r",
      "get words #1087\r",
      "get words #1088\r",
      "get words #1089\r",
      "get words #1090\r",
      "get words #1091\r",
      "get words #1092\r",
      "get words #1093\r",
      "get words #1094\r",
      "get words #1095\r",
      "get words #1096\r",
      "get words #1097\r",
      "get words #1098\r",
      "get words #1099\r",
      "get words #1100\r",
      "get words #1101\r",
      "get words #1102\r",
      "get words #1103\r",
      "get words #1104\r",
      "get words #1105\r",
      "get words #1106\r",
      "get words #1107\r",
      "get words #1108\r",
      "get words #1109\r",
      "get words #1110\r",
      "get words #1111\r",
      "get words #1112\r",
      "get words #1113\r",
      "get words #1114\r",
      "get words #1115\r",
      "get words #1116\r",
      "get words #1117\r",
      "get words #1118\r",
      "get words #1119\r",
      "get words #1120\r",
      "get words #1121\r",
      "get words #1122\r",
      "get words #1123\r",
      "get words #1124\r",
      "get words #1125\r",
      "get words #1126\r",
      "get words #1127\r",
      "get words #1128\r",
      "get words #1129\r",
      "get words #1130\r",
      "get words #1131\r",
      "get words #1132\r",
      "get words #1133\r",
      "get words #1134\r",
      "get words #1135\r",
      "get words #1136\r",
      "get words #1137\r",
      "get words #1138\r",
      "get words #1139\r",
      "get words #1140\r",
      "get words #1141\r",
      "get words #1142\r",
      "get words #1143\r",
      "get words #1144\r",
      "get words #1145\r",
      "get words #1146\r",
      "get words #1147\r",
      "get words #1148\r",
      "get words #1149\r",
      "get words #1150\r",
      "get words #1151\r",
      "get words #1152\r",
      "get words #1153\r",
      "get words #1154\r",
      "get words #1155\r",
      "get words #1156\r",
      "get words #1157\r",
      "get words #1158\r",
      "get words #1159\r",
      "get words #1160\r",
      "get words #1161\r",
      "get words #1162\r",
      "get words #1163\r",
      "get words #1164\r",
      "get words #1165\r",
      "get words #1166\r",
      "get words #1167\r",
      "get words #1168\r",
      "get words #1169\r",
      "get words #1170\r",
      "get words #1171\r",
      "get words #1172\r",
      "get words #1173\r",
      "get words #1174\r",
      "get words #1175\r",
      "get words #1176\r",
      "get words #1177\r",
      "get words #1178\r",
      "get words #1179\r",
      "get words #1180\r",
      "get words #1181\r",
      "get words #1182\r",
      "get words #1183\r",
      "get words #1184\r",
      "get words #1185\r",
      "get words #1186\r",
      "get words #1187\r",
      "get words #1188\r",
      "get words #1189\r",
      "get words #1190\r",
      "get words #1191\r",
      "get words #1192\r",
      "get words #1193\r",
      "get words #1194\r",
      "get words #1195\r",
      "get words #1196\r",
      "get words #1197\r",
      "get words #1198\r",
      "get words #1199\r",
      "get words #1200\r",
      "get words #1201\r",
      "get words #1202\r",
      "get words #1203\r",
      "get words #1204\r",
      "get words #1205\r",
      "get words #1206\r",
      "get words #1207\r",
      "get words #1208\r",
      "get words #1209\r",
      "get words #1210\r",
      "get words #1211\r",
      "get words #1212\r",
      "get words #1213\r",
      "get words #1214\r",
      "get words #1215\r",
      "get words #1216\r",
      "get words #1217\r",
      "get words #1218\r",
      "get words #1219\r",
      "get words #1220\r",
      "get words #1221\r",
      "get words #1222\r",
      "get words #1223\r",
      "get words #1224\r",
      "get words #1225\r",
      "get words #1226\r",
      "get words #1227\r",
      "get words #1228\r",
      "get words #1229\r",
      "get words #1230\r",
      "get words #1231\r",
      "get words #1232\r",
      "get words #1233\r",
      "get words #1234\r",
      "get words #1235\r",
      "get words #1236\r",
      "get words #1237\r",
      "get words #1238\r",
      "get words #1239\r",
      "get words #1240\r",
      "get words #1241\r",
      "get words #1242\r",
      "get words #1243\r",
      "get words #1244\r",
      "get words #1245\r",
      "get words #1246\r",
      "get words #1247\r",
      "get words #1248\r",
      "get words #1249\r",
      "get words #1250\r",
      "get words #1251\r",
      "get words #1252\r",
      "get words #1253\r",
      "get words #1254\r",
      "get words #1255\r",
      "get words #1256\r",
      "get words #1257\r",
      "get words #1258\r",
      "get words #1259\r",
      "get words #1260\r",
      "get words #1261\r",
      "get words #1262\r",
      "get words #1263\r",
      "get words #1264\r",
      "get words #1265\r",
      "get words #1266\r",
      "get words #1267\r",
      "get words #1268\r",
      "get words #1269\r",
      "get words #1270\r",
      "get words #1271\r",
      "get words #1272\r",
      "get words #1273\r",
      "get words #1274\r",
      "get words #1275\r",
      "get words #1276\r",
      "get words #1277\r",
      "get words #1278\r",
      "get words #1279\r",
      "get words #1280\r",
      "get words #1281\r",
      "get words #1282\r",
      "get words #1283\r",
      "get words #1284\r",
      "get words #1285\r",
      "get words #1286\r",
      "get words #1287\r",
      "get words #1288\r",
      "get words #1289\r",
      "get words #1290\r",
      "get words #1291\r",
      "get words #1292\r",
      "get words #1293\r",
      "get words #1294\r",
      "get words #1295\r",
      "get words #1296\r",
      "get words #1297\r",
      "get words #1298\r",
      "get words #1299\r",
      "get words #1300\r",
      "get words #1301\r",
      "get words #1302\r",
      "get words #1303\r",
      "get words #1304\r",
      "get words #1305\r",
      "get words #1306\r",
      "get words #1307\r",
      "get words #1308\r",
      "get words #1309\r",
      "get words #1310\r",
      "get words #1311\r",
      "get words #1312\r",
      "get words #1313\r",
      "get words #1314\r",
      "get words #1315\r",
      "get words #1316\r",
      "get words #1317\r",
      "get words #1318\r",
      "get words #1319\r",
      "get words #1320\r",
      "get words #1321\r",
      "get words #1322\r",
      "get words #1323\r",
      "get words #1324\r",
      "get words #1325\r",
      "get words #1326\r",
      "get words #1327\r",
      "get words #1328\r",
      "get words #1329\r",
      "get words #1330\r",
      "get words #1331\r",
      "get words #1332\r",
      "get words #1333\r",
      "get words #1334\r",
      "get words #1335\r",
      "get words #1336\r",
      "get words #1337\r",
      "get words #1338\r",
      "get words #1339\r",
      "get words #1340\r",
      "get words #1341\r",
      "get words #1342\r",
      "get words #1343\r",
      "get words #1344\r",
      "get words #1345\r",
      "get words #1346\r",
      "get words #1347\r",
      "get words #1348\r",
      "get words #1349\r",
      "get words #1350\r",
      "get words #1351\r",
      "get words #1352\r",
      "get words #1353\r",
      "get words #1354\r",
      "get words #1355\r",
      "get words #1356\r",
      "get words #1357\r",
      "get words #1358\r",
      "get words #1359\r",
      "get words #1360\r",
      "get words #1361\r",
      "get words #1362\r",
      "get words #1363\r",
      "get words #1364\r",
      "get words #1365\r",
      "get words #1366\r",
      "get words #1367\r",
      "get words #1368\r",
      "get words #1369\r",
      "get words #1370\r",
      "get words #1371\r",
      "get words #1372\r",
      "get words #1373\r",
      "get words #1374\r",
      "get words #1375\r",
      "get words #1376\r",
      "get words #1377\r",
      "get words #1378\r",
      "get words #1379\r",
      "get words #1380\r",
      "get words #1381\r",
      "get words #1382\r",
      "get words #1383\r",
      "get words #1384\r",
      "get words #1385\r",
      "get words #1386\r",
      "get words #1387\r",
      "get words #1388\r",
      "get words #1389\r",
      "get words #1390\r",
      "get words #1391\r",
      "get words #1392\r",
      "get words #1393\r",
      "get words #1394\r",
      "get words #1395\r",
      "get words #1396\r",
      "get words #1397\r",
      "get words #1398\r",
      "get words #1399\r",
      "get words #1400\r",
      "get words #1401\r",
      "get words #1402\r",
      "get words #1403\r",
      "get words #1404\r",
      "get words #1405\r",
      "get words #1406\r",
      "get words #1407\r",
      "get words #1408\r",
      "get words #1409\r",
      "get words #1410\r",
      "get words #1411\r",
      "get words #1412\r",
      "get words #1413\r",
      "get words #1414\r",
      "get words #1415\r",
      "get words #1416\r",
      "get words #1417\r",
      "get words #1418\r",
      "get words #1419\r",
      "get words #1420\r",
      "get words #1421\r",
      "get words #1422\r",
      "get words #1423\r",
      "get words #1424\r",
      "get words #1425\r",
      "get words #1426\r",
      "get words #1427\r",
      "get words #1428\r",
      "get words #1429\r",
      "get words #1430\r",
      "get words #1431\r",
      "get words #1432\r",
      "get words #1433\r",
      "get words #1434\r",
      "get words #1435\r",
      "get words #1436\r",
      "get words #1437\r",
      "get words #1438\r",
      "get words #1439\r",
      "get words #1440\r",
      "get words #1441\r",
      "get words #1442\r",
      "get words #1443\r",
      "get words #1444\r",
      "get words #1445\r",
      "get words #1446\r",
      "get words #1447\r",
      "get words #1448\r",
      "get words #1449\r",
      "get words #1450\r",
      "get words #1451\r",
      "get words #1452\r",
      "get words #1453\r",
      "get words #1454\r",
      "get words #1455\r",
      "get words #1456\r",
      "get words #1457\r",
      "get words #1458\r",
      "get words #1459\r",
      "get words #1460\r",
      "get words #1461\r",
      "get words #1462\r",
      "get words #1463\r",
      "get words #1464\r",
      "get words #1465\r",
      "get words #1466\r",
      "get words #1467\r",
      "get words #1468\r",
      "get words #1469\r",
      "get words #1470\r",
      "get words #1471\r",
      "get words #1472\r",
      "get words #1473\r",
      "get words #1474\r",
      "get words #1475\r",
      "get words #1476\r",
      "get words #1477\r",
      "get words #1478\r",
      "get words #1479\r",
      "get words #1480\r",
      "get words #1481\r",
      "get words #1482\r",
      "get words #1483\r",
      "get words #1484\r",
      "get words #1485\r",
      "get words #1486\r",
      "get words #1487\r",
      "get words #1488\r",
      "get words #1489\r",
      "get words #1490\r",
      "get words #1491\r",
      "get words #1492\r",
      "get words #1493\r",
      "get words #1494\r",
      "get words #1495\r",
      "get words #1496\r",
      "get words #1497\r",
      "get words #1498\r",
      "get words #1499\r",
      "get words #1500\r",
      "get words #1501\r",
      "get words #1502\r",
      "get words #1503\r",
      "get words #1504\r",
      "get words #1505\r",
      "get words #1506\r",
      "get words #1507\r",
      "get words #1508\r",
      "get words #1509\r",
      "get words #1510\r",
      "get words #1511\r",
      "get words #1512\r",
      "get words #1513\r",
      "get words #1514\r",
      "get words #1515\r",
      "get words #1516\r",
      "get words #1517\r",
      "get words #1518\r",
      "get words #1519\r",
      "get words #1520\r",
      "get words #1521\r",
      "get words #1522\r",
      "get words #1523\r",
      "get words #1524\r",
      "get words #1525\r",
      "get words #1526\r",
      "get words #1527\r",
      "get words #1528\r",
      "get words #1529\r",
      "get words #1530\r",
      "get words #1531\r",
      "get words #1532\r",
      "get words #1533\r",
      "get words #1534\r",
      "get words #1535\r",
      "get words #1536\r",
      "get words #1537\r",
      "get words #1538\r",
      "get words #1539\r",
      "get words #1540\r",
      "get words #1541\r",
      "get words #1542\r",
      "get words #1543\r",
      "get words #1544\r",
      "get words #1545\r",
      "get words #1546\r",
      "get words #1547\r",
      "get words #1548\r",
      "get words #1549\r",
      "get words #1550\r",
      "get words #1551\r",
      "get words #1552\r",
      "get words #1553\r",
      "get words #1554\r",
      "get words #1555\r",
      "get words #1556\r",
      "get words #1557\r",
      "get words #1558\r",
      "get words #1559\r",
      "get words #1560\r",
      "get words #1561\r",
      "get words #1562\r",
      "get words #1563\r",
      "get words #1564\r",
      "get words #1565\r",
      "get words #1566\r",
      "get words #1567\r",
      "get words #1568\r",
      "get words #1569\r",
      "get words #1570\r",
      "get words #1571\r",
      "get words #1572\r",
      "get words #1573\r",
      "get words #1574\r",
      "get words #1575\r",
      "get words #1576\r",
      "get words #1577\r",
      "get words #1578\r",
      "get words #1579\r",
      "get words #1580\r",
      "get words #1581\r",
      "get words #1582\r",
      "get words #1583\r",
      "get words #1584\r",
      "get words #1585\r",
      "get words #1586\r",
      "get words #1587\r",
      "get words #1588\r",
      "get words #1589\r",
      "get words #1590\r",
      "get words #1591\r",
      "get words #1592\r",
      "get words #1593\r",
      "get words #1594\r",
      "get words #1595\r",
      "get words #1596\r",
      "get words #1597\r",
      "get words #1598\r",
      "get words #1599\r",
      "get words #1600\r",
      "get words #1601\r",
      "get words #1602\r",
      "get words #1603\r",
      "get words #1604\r",
      "get words #1605\r",
      "get words #1606\r",
      "get words #1607\r",
      "get words #1608\r",
      "get words #1609\r",
      "get words #1610\r",
      "get words #1611\r",
      "get words #1612\r",
      "get words #1613\r",
      "get words #1614\r",
      "get words #1615\r",
      "get words #1616\r",
      "get words #1617\r",
      "get words #1618\r",
      "get words #1619\r",
      "get words #1620\r",
      "get words #1621\r",
      "get words #1622\r",
      "get words #1623\r",
      "get words #1624\r",
      "get words #1625\r",
      "get words #1626\r",
      "get words #1627\r",
      "get words #1628\r",
      "get words #1629\r",
      "get words #1630\r",
      "get words #1631\r",
      "get words #1632\r",
      "get words #1633\r",
      "get words #1634\r",
      "get words #1635\r",
      "get words #1636\r",
      "get words #1637\r",
      "get words #1638\r",
      "get words #1639\r",
      "get words #1640\r",
      "get words #1641\r",
      "get words #1642\r",
      "get words #1643\r",
      "get words #1644\r",
      "get words #1645\r",
      "get words #1646\r",
      "get words #1647\r",
      "get words #1648"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get words #55777\n",
      "total words: 55779\n",
      "sentence count #200000\n",
      "load model ...\n",
      "save csv ...\n",
      "Finish Predicting\n"
     ]
    }
   ],
   "source": [
    "# 開始測試模型並做預測\n",
    "print(\"loading testing data ...\")\n",
    "test_x = load_testing_data(testing_data)\n",
    "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "test_x = preprocess.sentence_word2idx()\n",
    "test_dataset = TwitterDataset(X=test_x, y=None)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 8)\n",
    "print('\\nload model ...')\n",
    "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
    "outputs = testing(batch_size, test_loader, model, device)\n",
    "\n",
    "# 寫到csv檔案供上傳kaggle\n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
    "print(\"save csv ...\")\n",
    "tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n",
    "print(\"Finish Predicting\")\n",
    "\n",
    "# 以下是使用command line上傳到kaggle的方式\n",
    "# 需要先pip install kaggle、Create API Token，詳細請看https://github.com/Kaggle/kaggle-api以及https://www.kaggle.com/code1110/how-to-submit-from-google-colab\n",
    "# kaggle competitions submit [competition-name] -f [csv file path]] -m [message]\n",
    "# ex: kaggle competitions submit ml-2020spring-hw4 -f output/predict.csv -m \"......\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iCyLSWFcEmP"
   },
   "source": [
    "### Run 20 epochs on n98\n",
    "real\t3m33.317s\n",
    "\n",
    "user\t3m29.813s\n",
    "\n",
    "sys\t1m9.469s"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "hw4_RNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
